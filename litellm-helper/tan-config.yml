model_list:
  - model_name: qwen2.5-0.5b-mlx
    litellm_params:
      model: lm_studio/qwen2.5-0.5b-instruct-mlx
      api_base: http://host.docker.internal:1234/v1
      api_key: not-needed

  - model_name: qwq-32b
    litellm_params:
      model: openrouter/qwen/qwq-32b:free
    model_info:
      supports_function_calling: true

  - model_name: DeepSeek-R1-Distill-Llama-70B
    litellm_params:
      model: together_ai/deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  - model_name: sambanova-R1-Distill-Llama
    litellm_params:
      model: sambanova/DeepSeek-R1-Distill-Llama-70B
  - model_name: gemini-2.0-flash
    litellm_params:
      model: openrouter/google/gemini-2.0-flash-lite-001
  - model_name: gemini-flash-1.5-8b
    litellm_params:
      model: openrouter/google/gemini-flash-1.5-8b
  - model_name: claude-3.7-sonnet
    litellm_params:
      model: openrouter/anthropic/claude-3.7-sonnet
      api_base: https://openrouter.ai/api/v1
    model_info:
      supports_function_calling: true
      supports_vision: true
  
  - model_name: Mistral-Saba
    litellm_params:
      model: openrouter/mistralai/mistral-saba
  - model_name: gpt-4o
    litellm_params:
      model: openrouter/openai/gpt-4o
    model_info:
      supports_function_calling: true
      supports_vision: true
  - model_name: sambanova-llama-90b-vision
    litellm_params:
      model: sambanova/Llama-3.2-90B-Vision-Instruct
    model_info:
      supports_vision: true
  - model_name: sambanova-llama-405b
    litellm_params:
      model: sambanova/Meta-Llama-3.1-405B-Instruct
  - model_name: sambanova-llama-70b
    litellm_params:
      model: sambanova/Meta-Llama-3.3-70B-Instruct
  - model_name: sambanova-qwen-72b
    litellm_params:
      model: sambanova/Qwen2.5-72B-Instruct
  - model_name: deepseek-v3
    litellm_params:
      model: together_ai/deepseek-ai/DeepSeek-V3
  - model_name: "open/*"
    litellm_params:
      model: "openrouter/*"
  - model_name: "sambanova/*" 
    litellm_params:
      model: "sambanova/*"
  - model_name: "together/*"
    litellm_params:
      model: "together_ai/*"
  - model_name: "lmstudio/*"
    litellm_params:
      model: "lm_studio/*"
      api_base: http://host.docker.internal:1234/v1
      api_key: not-needed

  - model_name: m2-bert-80M-8k
    litellm_params:
      model: together_ai/togethercomputer/m2-bert-80M-8k-retrieval

litellm_settings:
  drop_params: True
  num_retries: 3
  request_timeout: 300
  telemetry: False
  fallbacks: [{"*": ["gemini-2.0-flash"]}]
  context_window_fallbacks: [{"gemini-2.0-flash"}]
  redact_user_api_key_info: True
  set_verbose: False
  json_logs: true         # Get debug logs in json format
  success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env

general_settings: 
  alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env
  background_health_checks: false
  # disable_spend_logs: True    # Disable writing spend logs to DB
  # disable_error_logs: True    # Disable writing error logs to DB
